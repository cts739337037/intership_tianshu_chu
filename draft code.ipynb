{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSPC.set_index(pd.to_datetime(GSPC['Date']), inplace=True)#Set the index of the DataFrame to make it become date type.\n",
    "del GSPC['Date']\n",
    "GSPC=GSPC.T\n",
    "GSPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44740c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m pip install h5py\n",
    "from __future__ import print_function  \n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility  用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed()值，则每次生成的随即数都相同\n",
    "  \n",
    "from PIL import Image  \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD  \n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "'''\n",
    "Olivetti Faces是纽约大学的一个比较小的人脸库，由40个人的400张图片构成，即每个人的人脸图片为10张。每张图片的灰度级为8位，每个像素的灰度大小位于0-255之间。整张图片大小是1190 × 942，一共有20 × 20张照片。那么每张照片的大小就是（1190 / 20）× （942 / 20）= 57 × 47 。\n",
    "'''\n",
    "\n",
    "# There are 40 different classes  \n",
    "nb_classes = 40  # 40个类别\n",
    "epochs = 40  # 进行40轮次训\n",
    "batch_size = 40 # 每次迭代训练使用40个样本\n",
    "  \n",
    "# input image dimensions  \n",
    "img_rows, img_cols = 57, 47  \n",
    "# number of convolutional filters to use  \n",
    "nb_filters1, nb_filters2 = 5, 10  # 卷积核的数目（即输出的维度）\n",
    "# size of pooling area for max pooling  \n",
    "nb_pool = 2  \n",
    "# convolution kernel size  \n",
    "nb_conv = 3  # 单个整数或由两个整数构成的list/tuple，卷积核的宽度和长度。如为单个整数，则表示在各个空间维度的相同长度。\n",
    "  \n",
    "def load_data(dataset_path):  \n",
    "    img = Image.open(dataset_path)  \n",
    "    img_ndarray = np.asarray(img, dtype = 'float64') / 255  # asarray，将数据转化为np.ndarray，但使用原内存\n",
    "    # 400 pictures, size: 57*47 = 2679  \n",
    "    faces = np.empty((400, 2679)) \n",
    "    for row in range(20):  \n",
    "        for column in range(20):\n",
    "           faces[row * 20 + column] = np.ndarray.flatten(img_ndarray[row*57 : (row+1)*57, column*47 : (column+1)*47]) \n",
    "           # flatten将多维数组降为一维\n",
    "  \n",
    "    label = np.empty(400)  \n",
    "    for i in range(40):\n",
    "        label[i*10 : i*10+10] = i  \n",
    "    label = label.astype(np.int)  \n",
    "  \n",
    "    #train:320,valid:40,test:40  \n",
    "    train_data = np.empty((320, 2679))  \n",
    "    train_label = np.empty(320)  \n",
    "    valid_data = np.empty((40, 2679))  \n",
    "    valid_label = np.empty(40)  \n",
    "    test_data = np.empty((40, 2679))  \n",
    "    test_label = np.empty(40)  \n",
    "  \n",
    "    for i in range(40):\n",
    "        train_data[i*8 : i*8+8] = faces[i*10 : i*10+8] # 训练集中的数据\n",
    "        train_label[i*8 : i*8+8] = label[i*10 : i*10+8]  # 训练集对应的标签\n",
    "        valid_data[i] = faces[i*10+8] # 验证集中的数据\n",
    "        valid_label[i] = label[i*10+8] # 验证集对应的标签\n",
    "        test_data[i] = faces[i*10+9] \n",
    "        test_label[i] = label[i*10+9]   \n",
    "    \n",
    "    train_data = train_data.astype('float32')\n",
    "    valid_data = valid_data.astype('float32')\n",
    "    test_data = test_data.astype('float32')\n",
    "       \n",
    "    rval = [(train_data, train_label), (valid_data, valid_label), (test_data, test_label)]  \n",
    "    return rval  \n",
    "  \n",
    "def set_model(lr=0.005,decay=1e-6,momentum=0.9): \n",
    "    model = Sequential()\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        model.add(Conv2D(5, kernel_size=(3, 3), input_shape = (1, img_rows, img_cols)))\n",
    "    else:\n",
    "        model.add(Conv2D(5, kernel_size=(3, 3), input_shape = (img_rows, img_cols, 1)))\n",
    "    model.add(Activation('relu')) #sigmoid，relu，tanh，elu\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    model.add(Conv2D(10, kernel_size=(3, 3)))  \n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  \n",
    "    model.add(Dropout(0.25))  \n",
    "    model.add(Flatten())      \n",
    "    model.add(Dense(128)) #Full connection  \n",
    "    model.add(Activation('tanh')) \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(nb_classes))  \n",
    "    model.add(Activation('softmax'))  \n",
    "    sgd = SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)  \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd)#keras.losses.binary_crossentropy  'categorical_crossentropy'\n",
    "    return model  \n",
    "  \n",
    "def train_model(model,X_train, Y_train, X_val, Y_val):  \n",
    "    model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs,  \n",
    "          verbose=1, validation_data=(X_val, Y_val))  \n",
    "    model.save_weights('model_weights.h5', overwrite=True)  \n",
    "    return model  \n",
    "  \n",
    "def test_model(model,X,Y):  \n",
    "    model.load_weights('model_weights.h5')  \n",
    "    score = model.evaluate(X, Y, verbose=0)\n",
    "    return score  \n",
    "  \n",
    "if __name__ == '__main__':  \n",
    "    # the data, shuffled and split between tran and test sets  \n",
    "    (X_train, y_train), (X_val, y_val),(X_test, y_test) = load_data('olivettifaces.gif')  \n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)  \n",
    "        X_val = X_val.reshape(X_val.shape[0], 1, img_rows, img_cols)  \n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)  \n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)  \n",
    "        X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)  \n",
    "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)  \n",
    "        input_shape = (img_rows, img_cols, 1) # 1 为图像像素深度\n",
    "    \n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples') \n",
    "    print(X_val.shape[0], 'validate samples')  \n",
    "    print(X_test.shape[0], 'test samples')\n",
    "  \n",
    "    # convert class vectors to binary class matrices  \n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)  \n",
    "    Y_val = np_utils.to_categorical(y_val, nb_classes)  \n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)  \n",
    "  \n",
    "    model = set_model()\n",
    "    train_model(model, X_train, Y_train, X_val, Y_val)   \n",
    "    score = test_model(model, X_test, Y_test)  \n",
    "  \n",
    "    model.load_weights('model_weights.h5')  \n",
    "    classes = model.predict_classes(X_test, verbose=0)  \n",
    "    test_accuracy = np.mean(np.equal(y_test, classes))  \n",
    "    print(\"accuarcy:\", test_accuracy)\n",
    "    for i in range(0,40):\n",
    "        if y_test[i] != classes[i]:\n",
    "            print(y_test[i], '被错误分成', classes[i]);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4370ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "test_mm = mm.fit_transform(GSPC_test)\n",
    "Y_test_inv=mm.inverse_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val[0].shape\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(Y_val_df.apply(lambda x: x.sum()/15,axis=0),\"r\")\n",
    "plt.plot(X_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6a5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    ",scoring='f1_weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab9309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model by model = Sequential() with input shape [DATE_BACK,the number of features]\n",
    "LSTM_MODEL = None \n",
    "\n",
    "# Change Kreas model\n",
    "def declare_LSTM_MODEL(model=LSTM_MODEL):\n",
    "    print(\"LSTM_MODEL has changed to be %s and start your forecast.\"%model)\n",
    "    global LSTM_MODEL\n",
    "    LSTM_MODEL = model\n",
    "            \n",
    "\n",
    "# Build LSTM model\n",
    "def LSTM_model(shape):\n",
    "    if LSTM_MODEL is None:\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(CELLS*4, input_shape=(shape[1], shape[2]), activation='tanh', return_sequences=True))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(LSTM(CELLS*2,activation='tanh',return_sequences=True))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(LSTM(CELLS,activation='tanh',return_sequences=False))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(Dense(1,activation='tanh'))\n",
    "        model.compile(loss=OPTIMIZER_LOSS, optimizer='adam')\n",
    "        return model\n",
    "    elif LSTM_MODEL == 'GRU':\n",
    "        model = Sequential()\n",
    "        model.add(GRU(CELLS*4, input_shape=(shape[1], shape[2]), activation='tanh', return_sequences=True))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(GRU(CELLS*2,activation='tanh',return_sequences=True))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(GRU(CELLS,activation='tanh',return_sequences=False))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(Dense(1,activation='tanh'))\n",
    "        model.compile(loss=OPTIMIZER_LOSS, optimizer='adam')\n",
    "        return model\n",
    "    elif LSTM_MODEL == 'DNN':\n",
    "        model = Sequential()\n",
    "        model.add(Dense(CELLS*4, input_shape=(shape[1], shape[2]), activation='tanh'))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(Dense(CELLS*2,activation='tanh'))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(CELLS,activation='tanh'))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(Dense(1,activation='tanh'))\n",
    "        model.compile(loss=OPTIMIZER_LOSS, optimizer='adam')\n",
    "        return model\n",
    "    elif LSTM_MODEL == 'BPNN':\n",
    "        model = Sequential()\n",
    "        model.add(Dense(CELLS*4, input_shape=(shape[1], shape[2]), activation='tanh'))\n",
    "        model.add(Dropout(DROPOUT))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1,activation='tanh'))\n",
    "        model.compile(loss=OPTIMIZER_LOSS, optimizer='adam')\n",
    "        return model\n",
    "    else: return LSTM_MODEL\n",
    "\n",
    "# Other variables\n",
    "# -------------------------------\n",
    "# Method for unified normalization only 0,1,2,3\n",
    "METHOD = 0 \n",
    "\n",
    "# declare Method for unified normalization\n",
    "def declare_uni_method(method=None):\n",
    "    if method not in [0,1,2,3]: raise TypeError('METHOD should be 0,1,2,3.')\n",
    "    global METHOD\n",
    "    METHOD = method\n",
    "    print('Unified normalization method (%d) is start using.'%method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmf1_real(dataset1, dataset2,long_predict=10):\n",
    "    daily_log_rets1 = np.diff(np.log(dataset1).T)\n",
    "    daily_log_rets1=daily_log_rets1.reshape(len(dataset1)-1)\n",
    "    daily_rets_series1 = pd.DataFrame(daily_log_rets1)#calculate the log return\n",
    "    daily_log_rets2 = np.diff(np.log(dataset2).T)\n",
    "    daily_log_rets2=daily_log_rets2.reshape(len(dataset2)-1)\n",
    "    daily_rets_series2 = pd.DataFrame(daily_log_rets2)#calculate the log return\n",
    "    \n",
    "    f11=np.empty(0)\n",
    "    f12=np.empty(0)\n",
    "    \n",
    "    for n in range(0,1500,5):#get multiple threshold\n",
    "        threshold = n/100000\n",
    "        daily_label=np.empty(0)#make the data labelled\n",
    "        for i in range(0,len(daily_rets_series1)):\n",
    "            if daily_rets_series1.iloc[i,0]<-threshold:\n",
    "                n=-1\n",
    "                daily_label=np.append(daily_label,n)\n",
    "            elif abs(daily_rets_series1.iloc[i,0])<=threshold:\n",
    "                n=0\n",
    "                daily_label=np.append(daily_label,n)\n",
    "            else:\n",
    "                n=1\n",
    "                daily_label=np.append(daily_label,n)\n",
    "        \n",
    "        X1=pd.DataFrame()\n",
    "        X2=pd.DataFrame()\n",
    "        Y=pd.DataFrame()\n",
    "        for i in range(len(daily_rets_series1)-long_predict):\n",
    "            X1=pd.concat([X1, pd.DataFrame(daily_log_rets1[i:i+long_predict]).T],ignore_index=True)#n values of log return to predict trends\n",
    "            X2=pd.concat([X2, pd.DataFrame(daily_log_rets2[i:i+long_predict]).T],ignore_index=True)#n values of log return to predict trends\n",
    "            Y=pd.concat([Y, pd.DataFrame(daily_label[i+long_predict:i+long_predict+1]).T],ignore_index=True)#labels of trends\n",
    "        X1=np.array(X1)\n",
    "        X2=np.array(X2)\n",
    "        Y=np.array(Y[0])\n",
    "        \n",
    "        X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X1, Y, test_size=0.2,random_state=0)\n",
    "        svc_rbf1 = SVC(kernel='rbf',decision_function_shape='ovo')\n",
    "        f11=np.append(f11,cross_val_score(svc_rbf1 , X_train1, Y_train1, scoring='f1_micro',cv=5).mean())\n",
    "        \n",
    "        X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X2, Y, test_size=0.2,random_state=0)\n",
    "        svc_rbf2 = SVC(kernel='rbf',decision_function_shape='ovo')\n",
    "        f12=np.append(f12,cross_val_score(svc_rbf2 , X_train2, Y_train2, scoring='f1_micro',cv=5).mean())\n",
    "    return f11,f12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e06bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmf1_real_label(dataset1, dataset2,long_predict=10):\n",
    "    daily_log_rets1 = np.diff(np.log(dataset1).T)\n",
    "    daily_log_rets1=daily_log_rets1.reshape(len(dataset1)-1)\n",
    "    daily_rets_series1 = pd.DataFrame(daily_log_rets1)#calculate the log return\n",
    "    daily_log_rets2 = np.diff(np.log(dataset2).T)\n",
    "    daily_log_rets2=daily_log_rets2.reshape(len(dataset2)-1)\n",
    "    daily_rets_series2 = pd.DataFrame(daily_log_rets2)#calculate the log return\n",
    "    \n",
    "    f11=np.empty(0)\n",
    "    f12=np.empty(0)\n",
    "    \n",
    "    for n in np.arange(0.0, 0.04, 0.0005):#get multiple threshold\n",
    "        threshold = n\n",
    "        daily_label=np.empty(0)#make the data labelled\n",
    "        for i in range(0,len(daily_rets_series1)):\n",
    "            if daily_rets_series1.iloc[i,0]<-threshold:\n",
    "                n=-1\n",
    "                daily_label=np.append(daily_label,n)\n",
    "            elif abs(daily_rets_series1.iloc[i,0])<=threshold:\n",
    "                n=0\n",
    "                daily_label=np.append(daily_label,n)\n",
    "            else:\n",
    "                n=1\n",
    "                daily_label=np.append(daily_label,n)\n",
    "        \n",
    "        daily_label2=np.empty(0)#make the data labelled\n",
    "        for i in range(0,len(daily_rets_series2)):\n",
    "            if daily_rets_series2.iloc[i,0]<-threshold:\n",
    "                n=-1\n",
    "                daily_label2=np.append(daily_label2,n)\n",
    "            elif abs(daily_rets_series2.iloc[i,0])<=threshold:\n",
    "                n=0\n",
    "                daily_label2=np.append(daily_label2,n)\n",
    "            else:\n",
    "                n=1\n",
    "                daily_label2=np.append(daily_label2,n)\n",
    "        \n",
    "        X1=pd.DataFrame()\n",
    "        X2=pd.DataFrame()\n",
    "        Y=pd.DataFrame()\n",
    "        for i in range(len(daily_rets_series1)-long_predict):\n",
    "            X1=pd.concat([X1, pd.DataFrame(daily_label[i:i+long_predict]).T],ignore_index=True)#n values of log return to predict trends\n",
    "            X2=pd.concat([X2, pd.DataFrame(daily_label2[i:i+long_predict]).T],ignore_index=True)#n values of log return to predict trends\n",
    "            Y=pd.concat([Y, pd.DataFrame(daily_label[i+long_predict:i+long_predict+1]).T],ignore_index=True)#labels of trends\n",
    "        X1=np.array(X1)\n",
    "        X2=np.array(X2)\n",
    "        Y=np.array(Y[0])\n",
    "        \n",
    "        X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X1, Y, test_size=0.2,random_state=0)\n",
    "        svc_rbf1 = SVC(kernel='rbf',decision_function_shape='ovo')\n",
    "        f11=np.append(f11,cross_val_score(svc_rbf1 , X1, Y, scoring='f1_micro',cv=2).mean())\n",
    "        \n",
    "        X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X2, Y, test_size=0.2,random_state=0)\n",
    "        svc_rbf2 = SVC(kernel='rbf',decision_function_shape='ovo')\n",
    "        f12=np.append(f12,cross_val_score(svc_rbf2 , X2, Y, scoring='f1_micro',cv=2).mean())\n",
    "    return f11,f12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pure_mm_lstm_signal.index,pure_mm_lstm_signal.iloc[:,0])\n",
    "plt.scatter(pure_mm_lstm_signal.index,pure_mm_lstm_signal.iloc[:,1])\n",
    "plt.scatter(pure_mm_lstm_signal.index,pure_mm_lstm_signal.iloc[:,2])\n",
    "plt.scatter(pure_mm_lstm_signal.index,pure_mm_lstm_signal.iloc[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9987a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "close=np.array(GSPC)[0]\n",
    "GSPC_pure=pd.DataFrame()\n",
    "for i in range(2,52):#SMA\n",
    "    #print(i)\n",
    "    n=np.ones(i)\n",
    "    weights_sma=n/i\n",
    "    empty=np.ones(i-1)\n",
    "    sma=np.convolve(weights_sma, close, mode='valid')\n",
    "    sma_full=np.hstack((empty,sma))\n",
    "    GSPC_pure=pd.concat([GSPC_pure, pd.DataFrame(sma_full).T],ignore_index=True)\n",
    "    weights_ema=np.exp(np.linspace(0,1,i))\n",
    "    weights_ema =weights_ema/np.sum(weights_ema)\n",
    "    ema=np.convolve(weights_ema, close, mode='valid')\n",
    "    ema_full=np.hstack((empty,sma))\n",
    "    GSPC_pure=pd.concat([GSPC_pure, pd.DataFrame(ema_full).T],ignore_index=True)\n",
    "GSPC_pure=GSPC_pure.iloc[:, 52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility\n",
    "window_stdev = 20\n",
    "pure_mm_lstm2['volatility'] = pure_mm_lstm2['log_ret'].rolling(window=window_stdev).std().shift(1)\n",
    "\n",
    "\n",
    "window_autocorr = 20\n",
    "#compute the lag-N autocorrelation.\n",
    "pure_mm_lstm2['autocorr_1'] = pure_mm_lstm2['log_ret'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=1), raw=False).shift(1)\n",
    "pure_mm_lstm2['autocorr_2'] = pure_mm_lstm2['log_ret'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=2), raw=False).shift(1)\n",
    "pure_mm_lstm2['autocorr_3'] = pure_mm_lstm2['log_ret'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=3), raw=False).shift(1)\n",
    "pure_mm_lstm2['autocorr_4'] = pure_mm_lstm2['log_ret'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=4), raw=False).shift(1)\n",
    "pure_mm_lstm2['autocorr_5'] = pure_mm_lstm2['log_ret'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=5), raw=False).shift(1)\n",
    "pure_mm_lstm2['autocorr_6'] = pure_mm_lstm2['log_ret'].rolling(window=window_autocorr).apply(lambda x: x.autocorr(lag=6), raw=False).shift(1)\n",
    "\n",
    "# Get the various log -t returns\n",
    "pure_mm_lstm2['log_t1'] = pure_mm_lstm2['log_ret'].shift(1)\n",
    "pure_mm_lstm2['log_t2'] = pure_mm_lstm2['log_ret'].shift(2)\n",
    "pure_mm_lstm2['log_t3'] = pure_mm_lstm2['log_ret'].shift(3)\n",
    "pure_mm_lstm2['log_t4'] = pure_mm_lstm2['log_ret'].shift(4)\n",
    "pure_mm_lstm2['log_t5'] = pure_mm_lstm2['log_ret'].shift(5)\n",
    "\n",
    "# Add fast and slow moving averages\n",
    "fast_window = 12\n",
    "slow_window = 26\n",
    "\n",
    "pure_mm_lstm2['fast_mavg'] = pure_mm_lstm2['Close'].rolling(window=fast_window).mean().shift(1)\n",
    "pure_mm_lstm2['slow_mavg'] = pure_mm_lstm2['Close'].rolling(window=slow_window).mean().shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Trending signals  macd\n",
    "pure_mm_lstm2['sma'] = np.nan\n",
    "\n",
    "long_signals = pure_mm_lstm2['fast_mavg'] >= pure_mm_lstm2['slow_mavg']\n",
    "short_signals = pure_mm_lstm2['fast_mavg'] < pure_mm_lstm2['slow_mavg']\n",
    "pure_mm_lstm2.loc[long_signals, 'sma'] = 1\n",
    "pure_mm_lstm2.loc[short_signals, 'sma'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate short and long windows\n",
    "short_window = 12\n",
    "long_window = 26\n",
    "\n",
    "# Initialise the `signals` dataframeand add the `signal` column\n",
    "signals = pd.DataFrame(index=pure_mm_lstm2.index)\n",
    "signals['signal'] = 0.0\n",
    "\n",
    "# Calculate short term simple moving averages\n",
    "signals['short_mavg'] = pure_mm_lstm2['Close'].rolling(window=short_window, min_periods=1, center=False).mean()\n",
    "\n",
    "# Calculate long term simple moving averages\n",
    "signals['long_mavg'] = pure_mm_lstm2['Close'].rolling(window=long_window, min_periods=1, center=False).mean()\n",
    "\n",
    "# 生成信号\n",
    "signals['signal'][short_window:] = np.where(signals['short_mavg'][short_window:] \n",
    "                                            > signals['long_mavg'][short_window:], 1.0, 0.0)   \n",
    "\n",
    "# 生成交易命令\n",
    "signals['positions'] = signals['signal'].diff()\n",
    "\n",
    "# 输出`signals`\n",
    "signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "ax1 = fig.add_subplot(111,  ylabel='Price in $')\n",
    "\n",
    "# plot close price\n",
    "plt.plot(pure_mm_lstm2['Close'], color='r', lw=2,label='Close')\n",
    "# plot short/long_ma lines\n",
    "plt.plot(signals[['short_mavg', 'long_mavg']], lw=2,label=['short_mavg', 'long_mavg'])\n",
    "# 绘制买入信号\n",
    "ax1.plot(signals.loc[signals.positions == 1.0].index, \n",
    "         signals.short_mavg[signals.positions == 1.0],\n",
    "         '^', markersize=10, color='m',label='long')\n",
    "         \n",
    "# 绘制卖出信号\n",
    "ax1.plot(signals.loc[signals.positions == -1.0].index, \n",
    "         signals.short_mavg[signals.positions == -1.0],\n",
    "         'v', markersize=10, color='k',label='short')\n",
    "plt.legend()         \n",
    "# 显示做图\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
